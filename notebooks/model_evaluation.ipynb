{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   registration_form_id first_name_att         first_name_vf  \\\n",
      "0            6001285689          pedro               michael   \n",
      "2            6001287712           rora                  rosa   \n",
      "5         1010000005010   rosana david        rosaura daniel   \n",
      "7         1010000008304          mareo                 marco   \n",
      "8         1010000010099        micaela  gloria aurora agusti   \n",
      "\n",
      "         last_name_att last_name_vf  match_prob  \n",
      "0  hernandez rodriguez        hodge    0.000162  \n",
      "2                nunez        nunez    0.004387  \n",
      "5                 dome         dome    0.004387  \n",
      "7               vargas       vargas    0.004387  \n",
      "8        quentes elias      quevedo    0.000112  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_parquet_with_filter(parquet_path, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Load a Parquet file, filter rows by model_prob between 0 and 1, and keep only specified columns in order.\n",
    "\n",
    "    Args:\n",
    "        parquet_path (str): Path to the Parquet file.\n",
    "        columns_to_keep (list of str): List of column names to keep in the given order.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with specified columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    # Filter rows where match_prob is between 0 and 1\n",
    "    df_filtered = df[(df['match_prob'] >= 0.0) & (df['match_prob'] <= 0.50)]\n",
    "    # Filter only the columns requested, if they exist in the DataFrame\n",
    "    cols_to_use = [col for col in columns_to_keep if col in df_filtered.columns]\n",
    "    return df_filtered[cols_to_use]\n",
    "\n",
    "# Example usage:\n",
    "df = load_parquet_with_filter('/Users/borismartinez/Documents/GitHub/engage/predicted_best_matches_prototype.parquet', ['registration_form_id', 'first_name_att', 'first_name_vf', 'last_name_att', 'last_name_vf', 'match_prob'])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_preview_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mMedium Certainty\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m df_preview_all[\u001b[33m'\u001b[39m\u001b[33mcertainty_group\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_preview_all\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mmatch_prob\u001b[39m\u001b[33m'\u001b[39m].apply(certainty_category)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Columns for breakdown\u001b[39;00m\n\u001b[32m     21\u001b[39m categorical_cols = [\u001b[33m'\u001b[39m\u001b[33mparty_vf\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdob_year\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgender_att\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvoting_city\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'df_preview_all' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume df is your DataFrame loaded from Parquet or CSV containing 'match_prob' and the categorical columns\n",
    "\n",
    "# Define thresholds for high and low certainty\n",
    "high_threshold = 0.8\n",
    "low_threshold = 0.4\n",
    "\n",
    "# Create a new column categorizing match_prob\n",
    "def certainty_category(p):\n",
    "    if p >= high_threshold:\n",
    "        return 'High Certainty'\n",
    "    elif p <= low_threshold:\n",
    "        return 'Low Certainty'\n",
    "    else:\n",
    "        return 'Medium Certainty'\n",
    "\n",
    "df_preview_all['certainty_group'] = df_preview_all['match_prob'].apply(certainty_category)\n",
    "\n",
    "# Columns for breakdown\n",
    "categorical_cols = ['party_vf', 'dob_year', 'gender_att', 'voting_city']\n",
    "\n",
    "# Print count breakdowns for each categorical column by certainty group\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nBreakdown of {col} by certainty group:\")\n",
    "    breakdown = pd.crosstab(df['certainty_group'], df[col], margins=True, normalize='index') * 100\n",
    "    print(breakdown.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = (labeled_matches['match_prob'] >= thresh).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(labeled_matches['true_label'], preds)\n",
    "    acc = accuracy_score(labeled_matches['true_label'], preds)\n",
    "    prec = precision_score(labeled_matches['true_label'], preds)\n",
    "    rec = recall_score(labeled_matches['true_label'], preds)\n",
    "    f1 = f1_score(labeled_matches['true_label'], preds)\n",
    "    \n",
    "    print(f\"Threshold: {thresh}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve and AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, roc_thresholds = roc_curve(labeled_matches['true_label'], labeled_matches['match_prob'])\n",
    "auc_score = roc_auc_score(labeled_matches['true_label'], labeled_matches['match_prob'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_score:.4f})\")\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-Recall Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, pr_thresholds = precision_recall_curve(labeled_matches['true_label'], labeled_matches['match_prob'])\n",
    "avg_precision = average_precision_score(labeled_matches['true_label'], labeled_matches['match_prob'])\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, label=f\"Precision-Recall curve (AP = {avg_precision:.4f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(labeled_matches['true_label'], labeled_matches['match_prob'], n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Calibration curve')\n",
    "plt.plot([0,1], [0,1], 'k--', label='Perfectly calibrated')\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "brier_score = brier_score_loss(labeled_matches['true_label'], labeled_matches['match_prob'])\n",
    "print(f\"Brier score: {brier_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find threshold that maximizes F1 score\n",
    "best_thresh = None\n",
    "best_f1 = 0\n",
    "\n",
    "for thresh in np.linspace(0,1,101):\n",
    "    preds = (labeled_matches['match_prob'] >= thresh).astype(int)\n",
    "    f1 = f1_score(labeled_matches['true_label'], preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"Best threshold by F1 score: {best_thresh:.2f} with F1 = {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and insights\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
